{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f68225b-73f5-4798-b168-12fb5b408510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c32c85c3-6b07-4a7b-a096-45a268ffe7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = SparkSession.builder.appName(\"loading csv and json file\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2aea1f3f-d046-4988-8252-069dbe60ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"C:\\\\Users\\\\ASUS\\\\pyspark\\\\how to load csv and json files in pyspark\\\\example_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92aa5271-d297-41a2-93df-c57ae19cffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = app.read.csv(csv_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf0fb4fd-1de7-4163-a9df-32cbc59f9583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------+\n",
      "|column1|column2|   column3| column4|\n",
      "+-------+-------+----------+--------+\n",
      "|  Alice|     30|2024-11-01|12345.67|\n",
      "|    Bob|     25|2024-10-15|  890.12|\n",
      "+-------+-------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c13175b5-ba77-4eae-9f6c-0a6a50f4ec89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------+\n",
      "|column1|column2|   column3| column4|\n",
      "+-------+-------+----------+--------+\n",
      "|  Alice|     30|2024-11-01|12345.67|\n",
      "+-------+-------+----------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df.show(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7440f07-9f94-4e5d-b7ed-8e25af4a1aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(column1='Alice', column2=30, column3=datetime.date(2024, 11, 1), column4=12345.67),\n",
       " Row(column1='Bob', column2=25, column3=datetime.date(2024, 10, 15), column4=890.12)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b0f824e-9a63-40b0-b982-a35d2bcec91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- column1: string (nullable = true)\n",
      " |-- column2: integer (nullable = true)\n",
      " |-- column3: date (nullable = true)\n",
      " |-- column4: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3d91124-fa52-4dea-9180-a4e0484e5d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- join_date: date (nullable = true)\n",
      " |-- salary: decimal(10,2) (nullable = true)\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "+-----+---+----------+--------+\n",
      "| name|age| join_date|  salary|\n",
      "+-----+---+----------+--------+\n",
      "|Alice| 30|2024-11-01|12345.67|\n",
      "|  Bob| 25|2024-10-15|  890.12|\n",
      "+-----+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DecimalType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSVExample\").getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema_design = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"join_date\", DateType(), True),\n",
    "    StructField(\"salary\", DecimalType(10, 2), True)\n",
    "])\n",
    "\n",
    "# File path\n",
    "csv_path = r\"C:\\Users\\ASUS\\pyspark\\how to load csv and json files in pyspark\\example_data.csv\"\n",
    "\n",
    "# Load CSV file with schema\n",
    "csv_df1 = spark.read.csv(path=csv_path, schema=schema_design, header=True)\n",
    "\n",
    "# Print schema and data\n",
    "csv_df1.printSchema()\n",
    "print('--------------------------------------------------------------------')\n",
    "csv_df1.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
